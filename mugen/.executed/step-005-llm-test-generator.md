# Step 005: LLM Test Generator

## Status: NOT STARTED

| Task | Status | Date | Notes |
|------|--------|------|-------|
| Add nakama-ai dependency and initialize AI client | - | - | - |
| Define TestGenerationRequest and GeneratedTest structs | - | - | - |
| Implement context assembly (source, signatures, conventions) | - | - | - |
| Implement prompt templates per strategy (unit, property, mock, etc.) | - | - | - |
| Implement prompt templates per language (Rust, TS, Python, Go, Java) | - | - | - |
| Implement test code generation (LLM call, parse, clean) | - | - | - |
| Implement mock setup generation | - | - | - |
| Implement batch generation (multiple functions) | - | - | - |
| Add --model and --temperature flags | - | - | - |
| Unit tests with mocked LLM responses | - | - | - |
| Quality test: generated tests compile and are meaningful | - | - | - |

Status legend: `-` Not started | `WIP` In progress | `DONE` Complete | `BLOCKED` Blocked
